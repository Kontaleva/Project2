{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeUifMKGA_MV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d070d37e-823f-4de9-8c14-b21eae0ddd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J57QxylcBEg3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import json\n",
        "import html.parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pJ2pjin67HR"
      },
      "outputs": [],
      "source": [
        "#парсинг MIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udyj73bpkJaF"
      },
      "outputs": [],
      "source": [
        "all_page = ['https://news.mit.edu/topic/artificial-intelligence2?page=0', ]\n",
        "for i in range(1, 80):\n",
        "  all_page.append(f'https://news.mit.edu/topic/artificial-intelligence2?page={i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU-Ga9yQVsXF"
      },
      "outputs": [],
      "source": [
        "pur_url_lst = []\n",
        "\n",
        "for i in all_page:\n",
        "  html_text_page = requests.get(i).text\n",
        "  soup = BeautifulSoup(html_text_page, 'html.parser')\n",
        "  tag_url_artcl = soup.find_all('a', class_='term-page--news-article--item--title--link')\n",
        "  for i in tag_url_artcl:\n",
        "    pur_url_lst.append(f\"https://news.mit.edu{i['href']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VgHK-4OfAY7"
      },
      "outputs": [],
      "source": [
        "art_data_dict = {'headers': [], 'time': [], 'text': []}\n",
        "\n",
        "for art_url in pur_url_lst:\n",
        "\n",
        "  url_art_rquest = requests.get(art_url).text\n",
        "  soup_art = BeautifulSoup(url_art_rquest)\n",
        "  art_data_dict['headers'].append(soup_art.find('div', id=\"block-mit-page-title\").find('span').text)\n",
        "  art_data_dict['time'].append(soup_art.find('time').text)\n",
        "  art_data_dict['text'].append(soup_art.find('div', class_='news-article--content--body').text.replace('\\n', ' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in4gE73NwvzK"
      },
      "outputs": [],
      "source": [
        "with open('art_data.json', 'w', encoding='utf=8') as file:\n",
        "  json.dump(art_data_dict, file, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3GhthMAOTz0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json('art_data.json')\n",
        "df = pd.DataFrame(art_data_dict)\n",
        "df.to_csv('m_art_data.csv', sep=',', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ol_o9P6OZcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15a5f35-c693-4756-fc28-2a79af1e51fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                headers                time  \\\n",
            "0     MIT-Takeda Program wraps up with 16 publicatio...       June 18, 2024   \n",
            "1     Researchers leverage shadows to model 3D scene...       June 18, 2024   \n",
            "2     Understanding the visual knowledge of language...       June 17, 2024   \n",
            "3            A smarter way to streamline drug discovery       June 17, 2024   \n",
            "4     Technique improves the reasoning capabilities ...       June 14, 2024   \n",
            "...                                                 ...                 ...   \n",
            "1185                MIT's Robotic Fish Takes First Swim  September 13, 1994   \n",
            "1186             Can a robot teach us how people learn?       June 29, 1994   \n",
            "1187  Micro-robot holds promise for new surgical tec...        May 18, 1994   \n",
            "1188                  Robot journeys beneath arctic ice      April 13, 1994   \n",
            "1189                    'Manus' lends a hand in therapy      March 16, 1994   \n",
            "\n",
            "                                                   text  \n",
            "0        When the Takeda Pharmaceutical Co. and the ...  \n",
            "1        Imagine driving through a tunnel in an auto...  \n",
            "2        You’ve likely heard that a picture is worth...  \n",
            "3        The use of AI to streamline drug discovery ...  \n",
            "4        Large language models like those that power...  \n",
            "...                                                 ...  \n",
            "1185     CAMBRIDGE, Mass.--Consider the fish: highly...  \n",
            "1186     Cog, the newest and most ambitious robot de...  \n",
            "1187     A few years from now, patients with polyps ...  \n",
            "1188     Despite a wind chill index of -80oF and an ...  \n",
            "1189     MIT engineers have developed a robot that c...  \n",
            "\n",
            "[1190 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VniWMEUHzRKb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('m_art_data.csv')\n",
        "df.to_csv('m_art_data.txt', sep='\\t', index=False)\n",
        "\n",
        "df = df['headers']\n",
        "df.to_csv('m_headers.txt', sep='\\t', index=False)\n",
        "\n",
        "df = pd.read_csv('m_art_data.csv')\n",
        "df.to_csv('m_art_data.txt', sep='\\t', index=False)\n",
        "\n",
        "df = df['text']\n",
        "df.to_csv('m_text.txt', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2UtOFkj_fgB"
      },
      "outputs": [],
      "source": [
        "# парсинг The economist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjTYaJ4s7I8p"
      },
      "outputs": [],
      "source": [
        "all_page2 = ['https://www.economist.com/search?q=AI&page=1', ]\n",
        "for i in range(1, 10):\n",
        "  all_page2.append(f'https://www.economist.com/search?q=AI&page={i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J4ht9n0DxvN"
      },
      "outputs": [],
      "source": [
        "e_pur_url_set = set()\n",
        "\n",
        "for i in all_page2:\n",
        "  html_text_page = requests.get(i).text\n",
        "  soup = BeautifulSoup(html_text_page, 'html.parser')\n",
        "  e_tag_url_artcl = soup.find_all('a', class_='_search-result')\n",
        "\n",
        "  for i in e_tag_url_artcl:\n",
        "    e_pur_url_set.add(i['href'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlzzYyg0SA6k"
      },
      "outputs": [],
      "source": [
        "e_art_data_dict = {'headers': [], 'time': [], 'text': []}\n",
        "\n",
        "for e_art_url in e_pur_url_set:\n",
        "\n",
        "  try:\n",
        "    e_url_art_rquest = requests.get(e_art_url).text\n",
        "    soup_art = BeautifulSoup(e_url_art_rquest)\n",
        "    headers = soup_art.find('main', id=\"content\").find('div', class_='css-1r2sn2n e1mdktgm0').find('h1', class_=\"css-sous8d e1phhnog0\").text\n",
        "    e_art_data_dict['headers'].append(headers)\n",
        "\n",
        "    time = soup_art.find('time').text\n",
        "    e_art_data_dict['time'].append(time)\n",
        "\n",
        "    rex = re.search(r'css-tnm0wy ekpjo0f0|css-cbns87 ekpjo0f2', e_url_art_rquest)\n",
        "    text = soup_art.find('div', class_=rex.group(0)).text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "    e_art_data_dict['text'].append(text)\n",
        "  except AttributeError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9_rJUBhMWsj"
      },
      "outputs": [],
      "source": [
        "with open('e_art_data.json', 'w', encoding='utf=8') as file:\n",
        "  json.dump(e_art_data_dict, file, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RrbGuO8ZWQS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json('e_art_data.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHkw1LueEQQJ"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L3_u1uwP3bi"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(e_art_data_dict)\n",
        "e_df = pd.DataFrame(e_art_data_dict)\n",
        "e_df.to_csv('e_art_data.csv', sep=',', encoding='utf-8')\n",
        "\n",
        "e_df = pd.read_csv('e_art_data.csv')\n",
        "e_df.to_csv('e_art_data.txt', sep='\\t', index=False)\n",
        "\n",
        "print(e_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e_df = df['headers']\n",
        "e_df.to_csv('e_headers.txt', sep='\\t', index=False)\n",
        "\n",
        "e_df = pd.read_csv('e_art_data.csv')\n",
        "e_df.to_csv('e_art_data.txt', sep='\\t', index=False)\n",
        "\n",
        "e_df = df['text']\n",
        "e_df.to_csv('e_text.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "IzGc-PtULhuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J25SW6CWjTau"
      },
      "outputs": [],
      "source": [
        "# парсинг BBC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYBqFIgSjXSM"
      },
      "outputs": [],
      "source": [
        "page_lst = [f'https://www.bbc.co.uk/search?q=AI&d=SEARCH_PS&seqId=a377c420-d300-11ee-807f-ab7c7d65eaf8&page={i}' for i in range(1, 30)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDcekvIpHs4p"
      },
      "outputs": [],
      "source": [
        "b_art_url_lst = []\n",
        "\n",
        "for pg in page_lst:\n",
        "    html_txt_pg = requests.get(pg).text\n",
        "    soup = BeautifulSoup(html_txt_pg, 'html.parser')\n",
        "    srch_url = soup.find_all('a', class_='ssrcss-its5xf-PromoLink exn3ah91')\n",
        "\n",
        "    for i in srch_url:\n",
        "        b_art_url_lst.append(i['href'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnnA2ueNjg0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876cee3c-f3aa-4b1d-8f20-6879d5c3aaaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "290\n"
          ]
        }
      ],
      "source": [
        "pprint(len(b_art_url_lst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fAspP1Jj5EZ"
      },
      "outputs": [],
      "source": [
        "b_art_data_dict = {'headers': [], 'time': [], 'text': []}\n",
        "\n",
        "reg_comp = re.compile(r'\\d\\d\\d\\d-\\d\\d-\\d\\d')\n",
        "\n",
        "for art in b_art_url_lst:\n",
        "\n",
        "    try:\n",
        "        url_art_rquest = requests.get(art).text\n",
        "        soup_art = BeautifulSoup(url_art_rquest)\n",
        "        time = soup_art.find('span', class_='ssrcss-1if1g9v-MetadataText e4wm5bw1').find('time').get('datetime')\n",
        "        b_art_data_dict['time'].append(re.search(reg_comp, time).group(0))\n",
        "        b_art_data_dict['headers'].append(soup_art.find('h1', id=\"main-heading\").text)\n",
        "        txt = [p.text for p in soup_art.find_all('p', class_='ssrcss-1q0x1qg-Paragraph e1jhz7w10')]\n",
        "        b_art_data_dict['text'].append(str(txt))\n",
        "    except AttributeError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34NSmM4IkAHn"
      },
      "outputs": [],
      "source": [
        "with open('b_art_data.json', 'w', encoding='utf=8') as file:\n",
        "    json.dump(b_art_data_dict, file, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM4fL-Y0luOg"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json('b_art_data.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dpXcJuRl0rS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb2411e-3d81-41b3-f213-1dad39fdd8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Unnamed: 0                                            headers  \\\n",
            "0             0         How mobile phone networks are embracing AI   \n",
            "1             1  AI frenzy makes Nvidia the world's most valuab...   \n",
            "2             2  London cinema drops AI-written film after back...   \n",
            "3             3  Bacon ice cream and nugget overload sees misfi...   \n",
            "4             4  Tech firm creates AI devices to help aging pop...   \n",
            "..          ...                                                ...   \n",
            "184         184  AI used to deter deer from railway tracks over...   \n",
            "185         185     Community rallies to aid closure-threat church   \n",
            "186         186  Bollywood: How AI may affect India's vast film...   \n",
            "187         187  City aims to become UK's sculpture capital by ...   \n",
            "188         188  Bedfordshire Police use AI to save hours on ad...   \n",
            "\n",
            "           time                                               text  \n",
            "0    2024-06-19  ['Apple is due to rollout its new AI-powered o...  \n",
            "1    2024-06-18  ['Nvidia boss Jensen Huang has overseen explos...  \n",
            "2    2024-06-19  ['Soho\\'s Prince Charles Cinema dropped the sc...  \n",
            "3    2024-06-18  [\"McDonald's is removing artificial intelligen...  \n",
            "4    2024-06-18  ['The AI hob system warns people about potenti...  \n",
            "..          ...                                                ...  \n",
            "184  2023-12-20  [\"LNER and Network Rail's new initiative hopes...  \n",
            "185  2024-06-17  ['The tower at St Nicholas was built in 1500 b...  \n",
            "186  2023-12-18  ['Shah Rukh Khan was in an AI-enabled ad campa...  \n",
            "187  2024-06-17  ['The Hepworth art museum in Wakefield is name...  \n",
            "188  2023-12-14  ['Artificial Intelligence is helping Bedfordsh...  \n",
            "\n",
            "[189 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(b_art_data_dict)\n",
        "b_df = pd.DataFrame(b_art_data_dict)\n",
        "b_df.to_csv('b_art_data.csv', sep=',', encoding='utf-8')\n",
        "\n",
        "b_df = pd.read_csv('b_art_data.csv')\n",
        "b_df.to_csv('b_art_data.txt', sep='\\t', index=False)\n",
        "\n",
        "print(b_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kG3PsByGDTC"
      },
      "outputs": [],
      "source": [
        "b_df = df['headers']\n",
        "b_df.to_csv('b_headers.txt', sep='\\t', index=False)\n",
        "\n",
        "b_df = pd.read_csv('b_art_data.csv')\n",
        "b_df.to_csv('b_art_data.txt', sep='\\t', index=False)\n",
        "\n",
        "b_df = df['text']\n",
        "b_df.to_csv('b_text.txt', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# объединение заголовков и текстов The Economist & BBC"
      ],
      "metadata": {
        "id": "v871tkLgTJ1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"Media_headers.txt\",\"w\").write(open(\"e_headers.txt\",\"r\").read() + open(\"b_headers.txt\",\"r\").read())"
      ],
      "metadata": {
        "id": "3et-eLYJMYhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa0835d-9189-48f1-95ce-10a704e12e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14444"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"Media_text.txt\",\"w\").write(open(\"e_text.txt\",\"r\").read() + open(\"b_text.txt\",\"r\").read())"
      ],
      "metadata": {
        "id": "pjP_ULkcS77M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785da866-c713-4112-c6a5-b1f5d42fa0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1363533"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# парсинг fox news"
      ],
      "metadata": {
        "id": "NBCIZKgLrpz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('fox_pars_1-100.txt', 'r') as file:\n",
        "    article_html = file.read()"
      ],
      "metadata": {
        "id": "jEJksoFk1gUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_soup = BeautifulSoup(article_html, 'html.parser')"
      ],
      "metadata": {
        "id": "AUOSU7Hb1k80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "href_artic = fxn_soup.find_all('h2', class_=\"title\")"
      ],
      "metadata": {
        "id": "nM-qmjdA1oiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_artcl_url_lst = [i.find('a').get('href') for i in href_artic]"
      ],
      "metadata": {
        "id": "uvo12BDY1r3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_art_data_dict = {'headers': [], 'time': [], 'text': []}\n",
        "\n",
        "for url in fxn_artcl_url_lst:\n",
        "    try:\n",
        "        url_art_rquest = requests.get(url).text\n",
        "        soup_art = BeautifulSoup(url_art_rquest)\n",
        "        fxn_art_data_dict['time'].append(soup_art.find('span', class_=\"article-date\").find('time').text.strip())\n",
        "        fxn_art_data_dict['headers'].append(soup_art.find('h1', class_=\"headline speakable\").text)\n",
        "        txt = [i.text for i in soup_art.find('div', class_='article-body').find_all('p')]\n",
        "        fxn_art_data_dict['text'].append(str(txt))\n",
        "    except AttributeError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "rA66yu-_1x-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(fxn_art_data_dict)\n",
        "df.to_csv('fxn_art_data_1.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "G_KM17OR3B9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fxn_art_data_1.csv')\n",
        "df.to_csv('fxn_art_data_1.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "cVFhNqvz3O1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df['headers']\n",
        "df.to_csv('fxn_headers_1.txt', sep='\\t', index=False)\n",
        "\n",
        "df = pd.read_csv('fxn_art_data_1.csv')\n",
        "df.to_csv('fxn_art_data_1.txt', sep='\\t', index=False)\n",
        "\n",
        "df = df['text']\n",
        "df.to_csv('fxn_text_1.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "k2Iaq4pi3Pe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('fox_pars_100+.txt', 'r') as file:\n",
        "    article_html = file.read()"
      ],
      "metadata": {
        "id": "4YPfbBFGksFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_soup = BeautifulSoup(article_html, 'html.parser')"
      ],
      "metadata": {
        "id": "pIe1Z1RCBZ-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "href_artic = fxn_soup.find_all('h2', class_=\"title\")"
      ],
      "metadata": {
        "id": "e9Dl9pN4Bapi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_artcl_url_lst = [i.find('a').get('href') for i in href_artic]"
      ],
      "metadata": {
        "id": "IetJFkfTBcrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_art_data_dict = {'headers': [], 'time': [], 'text': []}\n",
        "\n",
        "for url in fxn_artcl_url_lst:\n",
        "    try:\n",
        "        url_art_rquest = requests.get(url).text\n",
        "        soup_art = BeautifulSoup(url_art_rquest)\n",
        "        fxn_art_data_dict['time'].append(soup_art.find('span', class_=\"article-date\").find('time').text.strip())\n",
        "        fxn_art_data_dict['headers'].append(soup_art.find('h1', class_=\"headline speakable\").text)\n",
        "        txt = [i.text for i in soup_art.find('div', class_='article-body').find_all('p')]\n",
        "        fxn_art_data_dict['text'].append(str(txt))\n",
        "    except AttributeError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "2vFDiRXYBfGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(fxn_art_data_dict)\n",
        "df.to_csv('fxn_art_data_2.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "pN0ioFf_FRbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fxn_art_data_2.csv')\n",
        "df.to_csv('fxn_art_data_2.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "VqqaulqsFR98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df['headers']\n",
        "df.to_csv('fxn_headers_2.txt', sep='\\t', index=False)\n",
        "\n",
        "df = pd.read_csv('fxn_art_data_2.csv')\n",
        "df.to_csv('fxn_art_data_2.txt', sep='\\t', index=False)\n",
        "\n",
        "df = df['text']\n",
        "df.to_csv('fxn_text_2.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "mDZ9wRhJFU3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('fox_pars_100++.txt', 'r') as file:\n",
        "    article_html = file.read()"
      ],
      "metadata": {
        "id": "yS8EXdTFF9gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_soup = BeautifulSoup(article_html, 'html.parser')"
      ],
      "metadata": {
        "id": "Q7hqUismF9Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "href_artic = fxn_soup.find_all('h2', class_=\"title\")"
      ],
      "metadata": {
        "id": "QSoKx_OeF9Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_artcl_url_lst = [i.find('a').get('href') for i in href_artic]"
      ],
      "metadata": {
        "id": "ML5UmpxTF9Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_art_data_dict = {'headers': [], 'time': [], 'text': []}\n",
        "\n",
        "for url in fxn_artcl_url_lst:\n",
        "    try:\n",
        "        url_art_rquest = requests.get(url).text\n",
        "        soup_art = BeautifulSoup(url_art_rquest)\n",
        "        fxn_art_data_dict['time'].append(soup_art.find('span', class_=\"article-date\").find('time').text.strip())\n",
        "        fxn_art_data_dict['headers'].append(soup_art.find('h1', class_=\"headline speakable\").text)\n",
        "        txt = [i.text for i in soup_art.find('div', class_='article-body').find_all('p')]\n",
        "        fxn_art_data_dict['text'].append(str(txt))\n",
        "    except AttributeError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "uM5NhPXrF9A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(fxn_art_data_dict)\n",
        "df.to_csv('fxn_art_data_3.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "5w3FNPC-F81r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fxn_art_data_3.csv')\n",
        "df.to_csv('fxn_art_data_3.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "tEzltTg1HC1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df['headers']\n",
        "df.to_csv('fxn_headers_3.txt', sep='\\t', index=False)\n",
        "\n",
        "df = pd.read_csv('fxn_art_data_3.csv')\n",
        "df.to_csv('fxn_art_data_3.txt', sep='\\t', index=False)\n",
        "\n",
        "df = df['text']\n",
        "df.to_csv('fxn_text_3.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "KM6c8clJHFtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('fox_pars_100+++.txt', 'r') as file:\n",
        "    article_html = file.read()"
      ],
      "metadata": {
        "id": "BISmF63dHO4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_soup = BeautifulSoup(article_html, 'html.parser')"
      ],
      "metadata": {
        "id": "YbSBXNikHWa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "href_artic = fxn_soup.find_all('h2', class_=\"title\")"
      ],
      "metadata": {
        "id": "UwBRWgqsHWQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_artcl_url_lst = [i.find('a').get('href') for i in href_artic]"
      ],
      "metadata": {
        "id": "m3wk6A6xHWId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_art_data_dict = {'headers': [], 'time': [], 'text': []}\n",
        "\n",
        "for url in fxn_artcl_url_lst:\n",
        "    try:\n",
        "        url_art_rquest = requests.get(url).text\n",
        "        soup_art = BeautifulSoup(url_art_rquest)\n",
        "        fxn_art_data_dict['time'].append(soup_art.find('span', class_=\"article-date\").find('time').text.strip())\n",
        "        fxn_art_data_dict['headers'].append(soup_art.find('h1', class_=\"headline speakable\").text)\n",
        "        txt = [i.text for i in soup_art.find('div', class_='article-body').find_all('p')]\n",
        "        fxn_art_data_dict['text'].append(str(txt))\n",
        "    except AttributeError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "iqitmOH2HV7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(fxn_art_data_dict)\n",
        "df.to_csv('fxn_art_data_4.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "1gmysgL_IIO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fxn_art_data_4.csv')\n",
        "df.to_csv('fxn_art_data_4.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "kPrSthBSII-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df['headers']\n",
        "df.to_csv('fxn_headers_4.txt', sep='\\t', index=False)\n",
        "\n",
        "df = pd.read_csv('fxn_art_data_4.csv')\n",
        "df.to_csv('fxn_art_data_4.txt', sep='\\t', index=False)\n",
        "\n",
        "df = df['text']\n",
        "df.to_csv('fxn_text_4.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "g1fNUC53IMxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('fox_pars_100++++.txt', 'r') as file:\n",
        "    article_html = file.read()"
      ],
      "metadata": {
        "id": "WFV3zSA1IhZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_soup = BeautifulSoup(article_html, 'html.parser')"
      ],
      "metadata": {
        "id": "IPk_0rNEIhNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "href_artic = fxn_soup.find_all('h2', class_=\"title\")"
      ],
      "metadata": {
        "id": "QyrLN0diIhGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_artcl_url_lst = [i.find('a').get('href') for i in href_artic]"
      ],
      "metadata": {
        "id": "ddW8qyM_IhAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fxn_art_data_dict = {'headers': [], 'time': [], 'text': []}\n",
        "\n",
        "for url in fxn_artcl_url_lst:\n",
        "    try:\n",
        "        url_art_rquest = requests.get(url).text\n",
        "        soup_art = BeautifulSoup(url_art_rquest)\n",
        "        fxn_art_data_dict['time'].append(soup_art.find('span', class_=\"article-date\").find('time').text.strip())\n",
        "        fxn_art_data_dict['headers'].append(soup_art.find('h1', class_=\"headline speakable\").text)\n",
        "        txt = [i.text for i in soup_art.find('div', class_='article-body').find_all('p')]\n",
        "        fxn_art_data_dict['text'].append(str(txt))\n",
        "    except AttributeError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "MROSQopUIg5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(fxn_art_data_dict)\n",
        "df.to_csv('fxn_art_data_5.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "dyQAmGmHIgxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fxn_art_data_5.csv')\n",
        "df.to_csv('fxn_art_data_5.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "p3OtH3RzIgoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df['headers']\n",
        "df.to_csv('fxn_headers_5.txt', sep='\\t', index=False)\n",
        "\n",
        "df = pd.read_csv('fxn_art_data_5.csv')\n",
        "df.to_csv('fxn_art_data_5.txt', sep='\\t', index=False)\n",
        "\n",
        "df = df['text']\n",
        "df.to_csv('fxn_text_5.txt', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "WbVHDaBdJ4R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# объединение датасета по fox news"
      ],
      "metadata": {
        "id": "Lh3J-CZOJ5JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"fxn_headers.txt\",\"w\").write(open(\"fxn_headers_1.txt\",\"r\").read() + open(\"fxn_headers_2.txt\",\"r\").read() +\n",
        "                               open(\"fxn_headers_3.txt\",\"r\").read() + open(\"fxn_headers_4.txt\",\"r\").read()\n",
        "                               + open(\"fxn_headers_5.txt\",\"r\").read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCTJaVcwKHCJ",
        "outputId": "cff1e8e6-69c3-49ee-b455-09a28dd81101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23579"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"fxn_text.txt\",\"w\").write(open(\"fxn_text_1.txt\",\"r\").read() + open(\"fxn_text_2.txt\",\"r\").read() +\n",
        "                               open(\"fxn_text_3.txt\",\"r\").read() + open(\"fxn_text_4.txt\",\"r\").read()\n",
        "                               + open(\"fxn_text_5.txt\",\"r\").read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhqeVbjJKG_d",
        "outputId": "7a50d5cb-f8d2-40a0-bc15-c5e038658417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1306490"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# объединение в один датасет для проверки на ML-моделях"
      ],
      "metadata": {
        "id": "hE_K_57oYIZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"AI_data_headers.txt\",\"w\").write(open(\"m_headers.txt\",\"r\").read() + open(\"Media_headers.txt\",\"r\").read() +\n",
        "                               open(\"fxn_headers.txt\",\"r\").read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgG5hOjzYIMA",
        "outputId": "ab9490a6-b699-45bd-d300-9d43feb76464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103791"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"AI_data._text.txt\",\"w\").write(open(\"m_text.txt\",\"r\").read() + open(\"Media_text.txt\",\"r\").read() +\n",
        "                               open(\"fxn_text.txt\",\"r\").read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXARV_4pZ0ou",
        "outputId": "c8539efe-ecac-4446-e8db-a0bac9ec3b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10079475"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('AI_data_headers.txt', 'r') as in_file:\n",
        "    stripped = (line.strip() for line in in_file)\n",
        "    lines = (line.split(\",\") for line in stripped if line)\n",
        "    with open('AI_data_headers.csv', 'w') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        writer.writerow(('title', 'intro'))\n",
        "        writer.writerows(lines)"
      ],
      "metadata": {
        "id": "15hHNEbTjXWL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}